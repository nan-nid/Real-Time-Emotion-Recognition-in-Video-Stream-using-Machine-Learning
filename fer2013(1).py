# -*- coding: utf-8 -*-
"""FER2013(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mkrCDlVGsaC2TjarqZu50z-qpy_6vCN-
"""

import pandas as pd  # Pandas for data manipulation and analysis
import numpy as np   # NumPy for numerical operations
import cv2 as cv     # OpenCV for image processing
import os            # OS module to interact with the operating system
import tensorflow as tf  # TensorFlow for machine learning models

# Additional imports
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For image data augmentation # Modified import statement
from tensorflow.keras.models import load_model  # To load a saved Keras model
from tensorflow.keras.models import Sequential  # For creating a sequential model # Modified import statement
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten  # Layers for model building # Modified import statement
from tensorflow.keras.optimizers import Adam  # Optimizer for training the model # Modified import statement

import warnings
import sys
if not sys.warnoptions:
    warnings.simplefilter("ignore")  # Ignore simple warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)  # Ignore deprecation warnings

fer2013 = pd.read_csv('/content/fer2013.csv')
fer2013.head()

fer2013['emotion'].value_counts()

training = fer2013[fer2013['Usage'] == 'Training']
training = training[training['emotion'] != 7]
training.drop('Usage', axis = 1, inplace = True)
training.reset_index(drop = True, inplace = True)
training

test = fer2013[fer2013['Usage'] == 'PrivateTest']
test = test[test['emotion'] != 7]
test.drop('Usage', axis = 1, inplace = True)
test.reset_index(drop = True, inplace = True)
test

def process_pixels(data):
    # Keep a copy of the emotion column
    emotion = data['emotion'].copy()

    # Check the type of the first element in the 'pixels' column to see if it's already a list
    if isinstance(data['pixels'].iloc[0], list):
        # If the pixels are already lists, just create a new DataFrame
        data = pd.DataFrame(data['pixels'].to_list(), columns=[f'pixel{i}' for i in range(len(data['pixels'].iloc[0]))])
    else:
        # If the pixels are strings, split them into lists of integers
        data['pixels'] = data['pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])
        data = pd.DataFrame(data['pixels'].to_list(), columns=[f'pixel{i}' for i in range(len(data['pixels'].iloc[0]))])

    # Add the emotion column back to the data
    data['emotion'] = emotion
    return data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Use the function to process the pixel data
training = process_pixels(training)

# Now proceed with the rest of your code
X = training.drop('emotion', axis=1)
y = training['emotion']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train = scaler.fit_transform(X_train)

# Transform the test data using the same scaler
X_test = scaler.transform(X_test)

"""## **SVM**"""

svc = SVC()
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)

# Print the accuracy
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')

def process_pixels1(data):

    if not isinstance(data['pixels'].iloc[0], list):

        data['pixels'] = data['pixels'].apply(lambda x: [int(pixel) for pixel in x.split()])

    pixel_data = pd.DataFrame(data['pixels'].to_list(), columns=[f'pixel{i}' for i in range(len(data['pixels'].iloc[0]))])

    # Returning the processed pixel data and corresponding emotion labels
    return pixel_data, data['emotion']
testingX, testingY = process_pixels1(test)
scaler1 = StandardScaler()
testingX = scaler1.fit_transform(testingX)
test_predict = svc.predict(testingX)
print(f'Accuracy: {accuracy_score(testingY, test_predict)}')

from sklearn.metrics import confusion_matrix, classification_report
import plotly.figure_factory as ff
report = classification_report(testingY, test_predict)
print(report)
cm = confusion_matrix(testingY, test_predict)
cm_df = pd.DataFrame(cm, index=[str(i) for i in range(cm.shape[0])], columns=[str(i) for i in range(cm.shape[1])])

fig = ff.create_annotated_heatmap(
    z=cm_df.values,  # The confusion matrix values
    x=list(cm_df.columns),  # Predicted labels (as column names)
    y=list(cm_df.index),  # True labels (as index)
    annotation_text=cm_df.values,  # Text to show on the heatmap (confusion matrix values)
    colorscale='Blues'  # Color scale of the heatmap
)
fig.update_layout(
    title='Confusion Matrix',
    xaxis_title='Predicted Label',
    yaxis_title='True Label'
)

# Displaying the plot
fig.show()

"""## **RC**"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train, y_train)
y_pred_rfc = rfc.predict(X_test)
print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred_rfc)}')

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train, y_train)

# Predictions
rfc_test_predict = rfc.predict(testingX)

# Print the accuracy
print(f'Random Forest Accuracy: {accuracy_score(testingY, rfc_test_predict)}')

# Classification report and confusion matrix
report_rfc = classification_report(testingY, rfc_test_predict)
print(report_rfc)

cm_rfc = confusion_matrix(testingY, rfc_test_predict)
cm_df_rfc = pd.DataFrame(cm_rfc, index=[str(i) for i in range(cm_rfc.shape[0])], columns=[str(i) for i in range(cm_rfc.shape[1])])

fig_rfc = ff.create_annotated_heatmap(
    z=cm_df_rfc.values,
    x=list(cm_df_rfc.columns),
    y=list(cm_df_rfc.index),
    annotation_text=cm_df_rfc.values,
    colorscale='Blues'
)
fig_rfc.update_layout(title='Random Forest Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')
fig_rfc.show()

"""## **KNN**"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print(f'K-Nearest Neighbors Accuracy: {accuracy_score(y_test, y_pred_knn)}')

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Predictions
knn_test_predict = knn.predict(testingX)

# Print the accuracy
print(f'K-Nearest Neighbors Accuracy: {accuracy_score(testingY, knn_test_predict)}')

# Classification report and confusion matrix
report_knn = classification_report(testingY, knn_test_predict)
print(report_knn)

cm_knn = confusion_matrix(testingY, knn_test_predict)
cm_df_knn = pd.DataFrame(cm_knn, index=[str(i) for i in range(cm_knn.shape[0])], columns=[str(i) for i in range(cm_knn.shape[1])])

fig_knn = ff.create_annotated_heatmap(
    z=cm_df_knn.values,
    x=list(cm_df_knn.columns),
    y=list(cm_df_knn.index),
    annotation_text=cm_df_knn.values,
    colorscale='Blues'
)
fig_knn.update_layout(title='KNN Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')
fig_knn.show()

"""## **LR**"""

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
print(f'Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_log_reg)}')

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

# Predictions
log_reg_test_predict = log_reg.predict(testingX)

# Print the accuracy
print(f'Logistic Regression Accuracy: {accuracy_score(testingY, log_reg_test_predict)}')

# Classification report and confusion matrix
report_log_reg = classification_report(testingY, log_reg_test_predict)
print(report_log_reg)

cm_log_reg = confusion_matrix(testingY, log_reg_test_predict)
cm_df_log_reg = pd.DataFrame(cm_log_reg, index=[str(i) for i in range(cm_log_reg.shape[0])], columns=[str(i) for i in range(cm_log_reg.shape[1])])

fig_log_reg = ff.create_annotated_heatmap(
    z=cm_df_log_reg.values,
    x=list(cm_df_log_reg.columns),
    y=list(cm_df_log_reg.index),
    annotation_text=cm_df_log_reg.values,
    colorscale='Blues'
)
fig_log_reg.update_layout(title='Logistic Regression Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')
fig_log_reg.show()

"""## **DL**"""

import tensorflow as tf

cnn_model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7, activation='softmax')  # Assuming 7 emotion classes
])

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# You will need to reshape X_train and X_test to match the input shape and convert y_train and y_test to categorical
X_train_cnn = X_train.reshape(-1, 48, 48, 1)
X_test_cnn = X_test.reshape(-1, 48, 48, 1)
y_train_cnn = tf.keras.utils.to_categorical(y_train)
y_test_cnn = tf.keras.utils.to_categorical(y_test)

history = cnn_model.fit(X_train_cnn, y_train_cnn, validation_data=(X_test_cnn, y_test_cnn), epochs=30, batch_size=64)

import tensorflow as tf

cnn_model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7, activation='softmax')  # Assuming 7 emotion classes
])

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

X_train_cnn = X_train.reshape(-1, 48, 48, 1)
X_test_cnn = testingX.reshape(-1, 48, 48, 1)
y_train_cnn = tf.keras.utils.to_categorical(y_train)
y_test_cnn = tf.keras.utils.to_categorical(testingY)

history = cnn_model.fit(X_train_cnn, y_train_cnn, validation_data=(X_test_cnn, y_test_cnn), epochs=30, batch_size=64)

# Predictions
cnn_test_predict = cnn_model.predict(X_test_cnn).argmax(axis=1)

# Print the accuracy
print(f'CNN Accuracy: {accuracy_score(testingY, cnn_test_predict)}')

# Classification report and confusion matrix
report_cnn = classification_report(testingY, cnn_test_predict)
print(report_cnn)

cm_cnn = confusion_matrix(testingY, cnn_test_predict)
cm_df_cnn = pd.DataFrame(cm_cnn, index=[str(i) for i in range(cm_cnn.shape[0])], columns=[str(i) for i in range(cm_cnn.shape[1])])

fig_cnn = ff.create_annotated_heatmap(
    z=cm_df_cnn.values,
    x=list(cm_df_cnn.columns),
    y=list(cm_df_cnn.index),
    annotation_text=cm_df_cnn.values,
    colorscale='Blues'
)
fig_cnn.update_layout(title='CNN Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')
fig_cnn.show()

"""## **GRAPHICAL REPRESENTATION**

## SVM
"""

import matplotlib.pyplot as plt
import numpy as np

# Sample data (replace with your actual values)
metrics = {
    'Accuracy': 0.434,
    'Precision': 0.44,
    'Recall': 0.43,
    'F1 Score': 0.42
}

# Extract metric names and values
names = list(metrics.keys())
values = list(metrics.values())

# Define the position of bars on x-axis
x_pos = np.arange(len(names))

# Plot the bars
plt.figure(figsize=(10, 6))
bars = plt.bar(x_pos, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightpink'])

# Add the values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', va='bottom')

# Customize the plot
plt.ylim(0, 1)
plt.xticks(x_pos, names)
plt.ylabel('Scores')
plt.title('Support Vector Machine')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Sample data (replace with your actual values)
metrics = {
    'Accuracy': 0.45,
    'Precision': 0.46,
    'Recall': 0.45,
    'F1 Score': 0.43
}

# Extract metric names and values
names = list(metrics.keys())
values = list(metrics.values())

# Define the position of bars on x-axis
x_pos = np.arange(len(names))

# Plot the bars
plt.figure(figsize=(10, 6))
bars = plt.bar(x_pos, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightpink'])

# Add the values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', va='bottom')

# Customize the plot
plt.ylim(0, 1)
plt.xticks(x_pos, names)
plt.ylabel('Scores')
plt.title('Random Classifier')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Sample data (replace with your actual values)
metrics = {
    'Accuracy': 0.32,
    'Precision': 0.33,
    'Recall': 0.33,
    'F1 Score': 0.33
}

# Extract metric names and values
names = list(metrics.keys())
values = list(metrics.values())

# Define the position of bars on x-axis
x_pos = np.arange(len(names))

# Plot the bars
plt.figure(figsize=(10, 6))
bars = plt.bar(x_pos, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightpink'])

# Add the values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', va='bottom')

# Customize the plot
plt.ylim(0, 1)
plt.xticks(x_pos, names)
plt.ylabel('Scores')
plt.title('KNN')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Sample data (replace with your actual values)
metrics = {
    'Accuracy': 0.35,
    'Precision': 0.35,
    'Recall': 0.36,
    'F1 Score': 0.35
}

# Extract metric names and values
names = list(metrics.keys())
values = list(metrics.values())

# Define the position of bars on x-axis
x_pos = np.arange(len(names))

# Plot the bars
plt.figure(figsize=(10, 6))
bars = plt.bar(x_pos, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightpink'])

# Add the values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', va='bottom')

# Customize the plot
plt.ylim(0, 1)
plt.xticks(x_pos, names)
plt.ylabel('Scores')
plt.title('Logistic Regression')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Sample data (replace with your actual values)
metrics = {
    'Accuracy': 0.45,
    'Precision': 0.47,
    'Recall': 0.46,
    'F1 Score': 0.46
}

# Extract metric names and values
names = list(metrics.keys())
values = list(metrics.values())

# Define the position of bars on x-axis
x_pos = np.arange(len(names))

# Plot the bars
plt.figure(figsize=(10, 6))
bars = plt.bar(x_pos, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightpink'])

# Add the values on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', va='bottom')

# Customize the plot
plt.ylim(0, 1)
plt.xticks(x_pos, names)
plt.ylabel('Scores')
plt.title('Deep Learning')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Display the plot
plt.show()